

# TGDD Scraping Documents:
A simple API built with FastAPI and PostgreSQL, being containerized using Docker. The main goal is to scraping products.

> [!IMPORTANT]
> [Docker is must have software installed in your computer to set up this project.](https://www.docker.com/)


The websites in which product data are being scraped:

### ğŸŒ Sources:

- [https://www.thegioididong.com/dtdd](https://www.thegioididong.com/dtdd)
- [https://www.thegioididong.com/laptop](https://www.thegioididong.com/laptop)

## ğŸ› ï¸ How It's Made:

### Programming & Markup Languages

ğŸ [Python](https://www.python.org/)
ğŸ”„ [TypeScript](https://www.typescriptlang.org/)
ğŸŒ [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML)
ğŸ¨ [CSS](https://developer.mozilla.org/en-US/docs/Web/CSS)

### Core Tools:

ğŸ˜ [PostgreSQL](https://www.postgresql.org/)

ğŸ³ [Docker](https://www.docker.com/) - Used for isolating services and encapsulating necessary things for the software instead of doing it 
traditional ways.

ğŸ§­ [NGINX](https://nginx.org/) - Acts as a gateway that routes frontend and backend services under a unified interface.

ğŸ§­ [NGINX](https://nginx.org/) - Acts as a gateway that routes frontend and backend services under a unified interface.

ğŸ§­ [NGINX](https://nginx.org/) - Acts as a gateway that routes frontend and backend services under a unified interface.

### Core Packages:

âš¡ [FastAPI](https://fastapi.tiangolo.com/) - Used for building microservices for scraping and inserting data.

âš›ï¸ [React](https://react.dev/) - Used for building dynamic front-end.

ğŸ§ª [SQLAlchemy](https://www.sqlalchemy.org/) - Used for opening connections towards the database engine and performing queries.

âœ… [Pydantic](https://docs.pydantic.dev/latest/) - Used for validating incoming data through API endpoints.

### Web Scraping Packages

ğŸœ [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - Used for catching static html tags.

ğŸ¤– [Selenium](https://www.selenium.dev/documentation/) - Used for automated browsing and handling dynamic content.

ğŸŒ [Requests](https://requests.readthedocs.io/en/latest/) - Used for making HTTP(or HTTPS) to certain domains. In this particular topic [Sources](#tgdd-scraping-documents).

### Minor Packages:

ğŸ“ [Logging](https://docs.python.org/3/library/logging.html) - Used for writing logs for examining the results.

## ğŸ“˜ Tasks List:
- âœ… Data Crawling API Serivce
- âœ… Data Ingestion API Service
- âœ… Database API Service
- âœ… Web Page Service
- âœ… Database Service
- âœ… nginx Service
- âœ… Logging
- âœ… Containerized Services
 
## ğŸ› ï¸ Instructions:

### ğŸ’¾ Clone the Repository

1. git clone https://github.com/vinhthai2905/TGDD_Scraping.git
2. cd TGDD_Scraping/base

### ğŸ³ Running with Docker Compose

3. docker-compose -f ./docker-compose.yml -p tgdd up --build

### ğŸ” Examining logs

> [!TIP] 
> Open three terminal and type in each
 - docker attach data_crawling
 - docker attach data_ingestion
 - docker attach database_api

### ğŸ›‘ Stop the service

5. docker-compose -f./docker-compose.yml -p tgdd down

## ğŸ’¡ Interactive Services Routed through [NGINX](https://nginx.org/)

### Web Page Service Container

![Screenshot of UI](assets/nginx.png)

### Data Crawling API Service Container

![Screenshot of UI](assets/8000.png)

### Data Ingestion API Service Container

![Screenshot of UI](assets/8001.png)

### Database API Service Container

![Screenshot of UI](assets/8002.png)

## ğŸ“ Navigators

â¡ï¸ [Go to Scraping Documents](#tgdd-scraping-documents)

â¡ï¸ [Go to How It's Made](#how-its-made)

â¡ï¸ [Go to How Tasks List](#tasks-list)

â¡ï¸ [Go to Instructions](#instructions)
